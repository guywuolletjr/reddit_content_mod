{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import string\n",
    "import datetime\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#modeling imports\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#tf imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score\n",
    "\n",
    "\n",
    "#multinomial nb\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from scipy.sparse.linalg import svds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "subm = pd.read_csv('./data/sample_submission.csv')\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['none'] = 1-train[labels].max(axis=1) #make an indicator for when there is no \n",
    "                                            #value for any of the labels \n",
    "#cant have any of the unknown values \n",
    "train['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "test['comment_text'].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  none  \n",
       "0             0        0       0       0              0     1  \n",
       "1             0        0       0       0              0     1  \n",
       "2             0        0       0       0              0     1  \n",
       "3             0        0       0       0              0     1  \n",
       "4             0        0       0       0              0     1  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    data = os.path.join(\"data\", \"train.csv\")\n",
    "\n",
    "    df = pd.read_csv(data)\n",
    "    X_train = df[['comment_text']]\n",
    "    y_train = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "    X_test = pd.read_csv(os.path.join(\"data\", \"test.csv\"))\n",
    "    y_test = pd.read_csv(os.path.join(\"data\", \"test_labels.csv\"))\n",
    "    test = X_test.merge(y_test, on='id')\n",
    "    test = test[ (test['toxic']!=-1) | (test['severe_toxic']!=-1) | \n",
    "                (test['obscene']!=-1) | (test['threat']!=-1) | (test['insult']!=-1) \n",
    "                | (test['identity_hate']!=-1) ]\n",
    "    test = test.reset_index(drop=True) \n",
    "    \n",
    "    X_test = test[['comment_text']]\n",
    "    y_test = test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Isabella_GC/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "#lemmatizing\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "BLACKLIST_STOPWORDS = ['over','only','very','not','no']\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english')) - set(BLACKLIST_STOPWORDS)\n",
    "import re\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to lowercase\n",
    "X_train = X_train.apply(lambda x: x.astype(str).str.lower())\n",
    "X_test = X_test.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "#column now has the expanded contractions\n",
    "X_train['expanded'] = X_train.comment_text.apply(expandContractions)\n",
    "X_test['expanded'] = X_test.comment_text.apply(expandContractions)\n",
    "\n",
    "#remove numbers, punctuation\n",
    "#https://medium.com/@chaimgluck1/have-messy-text-data-clean-it-with-simple-lambda-functions-645918fcc2fc\n",
    "X_train['expanded'] = X_train.expanded.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "X_test['expanded'] = X_test.expanded.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "X_train['expanded'] = X_train.expanded.apply(lambda x: x.translate(str.maketrans('','','1234567890')))\n",
    "X_test['expanded'] = X_test.expanded.apply(lambda x: x.translate(str.maketrans('','','1234567890')))\n",
    "\n",
    "#take away the \\n vals\n",
    "X_train['expanded'] = X_train.expanded.apply(lambda x: x.translate(str.maketrans('\\n',' ')))\n",
    "X_test['expanded'] = X_test.expanded.apply(lambda x: x.translate(str.maketrans('\\n',' ')))\n",
    "\n",
    "#remove english stop words\n",
    "\n",
    "#strip white space\n",
    "\n",
    "#lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.drop(['new_lines'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>expanded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>hey man im really not trying to edit war it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>more i cannot make any real suggestions on im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::and for the second time of asking, when ...</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>you should be ashamed of yourself \\n\\nthat is ...</td>\n",
       "      <td>you should be ashamed of yourself   that is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>spitzer \\n\\numm, theres no actual article for ...</td>\n",
       "      <td>spitzer   umm theres no actual article for pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nand ... i really don't think you understand...</td>\n",
       "      <td>and  i really do not think you understand  i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  \\\n",
       "0       explanation\\nwhy the edits made under my usern...   \n",
       "1       d'aww! he matches this background colour i'm s...   \n",
       "2       hey man, i'm really not trying to edit war. it...   \n",
       "3       \"\\nmore\\ni can't make any real suggestions on ...   \n",
       "4       you, sir, are my hero. any chance you remember...   \n",
       "...                                                   ...   \n",
       "159566  \":::::and for the second time of asking, when ...   \n",
       "159567  you should be ashamed of yourself \\n\\nthat is ...   \n",
       "159568  spitzer \\n\\numm, theres no actual article for ...   \n",
       "159569  and it looks like it was actually you who put ...   \n",
       "159570  \"\\nand ... i really don't think you understand...   \n",
       "\n",
       "                                                 expanded  \n",
       "0       explanation why the edits made under my userna...  \n",
       "1       daww he matches this background colour im seem...  \n",
       "2       hey man im really not trying to edit war it is...  \n",
       "3        more i cannot make any real suggestions on im...  \n",
       "4       you sir are my hero any chance you remember wh...  \n",
       "...                                                   ...  \n",
       "159566  and for the second time of asking when your vi...  \n",
       "159567  you should be ashamed of yourself   that is a ...  \n",
       "159568  spitzer   umm theres no actual article for pro...  \n",
       "159569  and it looks like it was actually you who put ...  \n",
       "159570   and  i really do not think you understand  i ...  \n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "import keras\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences #padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: should eventually take out the stop words here\n",
    "#train tokenizer, then encode documents (comment_text)\n",
    "tokenizer = Tokenizer(oov_token=True)\n",
    "tokenizer.fit_on_texts(X_train['comment_text']) #fit the tokenizer to training set, make the test unknown words be an UNK value.\n",
    "#trying to find the length of the vocabulary, might shrink\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "num_words\n",
    "#looks like there is about 210K words... maybe we should pass in a maxwords \n",
    "#argument to the ttokenizer, but my guess is words that are really making \n",
    "#toxic comments are less frequent, so don't necessarily want to do that...\n",
    "sequences = tokenizer.texts_to_sequences(X_train['comment_text'])\n",
    "tokens = tokenizer.sequences_to_texts(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=150)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test['comment_text'])\n",
    "test_data = pad_sequences(test_sequences, maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network architecture\n",
    "# inspired at https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e and https://medium.com/@sabber/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b\n",
    "model = Sequential()\n",
    "#first layer is embedding, takes in size of vocab, 100 dim embedding, and 150 which is length of the comment \n",
    "model.add(Embedding(num_words, 100, input_length=150)) \n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(8, activation='sigmoid'))#change to 8 \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0           0             0        0       0       0              0\n",
       "1           0             0        0       0       0              0\n",
       "2           0             0        0       0       0              0\n",
       "3           0             0        0       0       0              0\n",
       "4           0             0        0       0       0              0\n",
       "...       ...           ...      ...     ...     ...            ...\n",
       "159566      0             0        0       0       0              0\n",
       "159567      0             0        0       0       0              0\n",
       "159568      0             0        0       0       0              0\n",
       "159569      0             0        0       0       0              0\n",
       "159570      0             0        0       0       0              0\n",
       "\n",
       "[159571 rows x 6 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Isabella_GC/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/3\n",
      "111699/111699 [==============================] - 1872s 17ms/step - loss: 0.1388 - acc: 0.9529 - val_loss: 0.1044 - val_acc: 0.9621\n",
      "Epoch 2/3\n",
      "111699/111699 [==============================] - 2025s 18ms/step - loss: 0.0792 - acc: 0.9698 - val_loss: 0.1018 - val_acc: 0.9638\n",
      "Epoch 3/3\n",
      "111699/111699 [==============================] - 1793s 16ms/step - loss: 0.0533 - acc: 0.9795 - val_loss: 0.1214 - val_acc: 0.9602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2f46a400>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first run through didn't specify a batch size, probably do that\n",
    "#on the next try. \n",
    "model.fit(data, np.array(y_train['toxic']), validation_split=.3, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model_toxic to disk\n"
     ]
    }
   ],
   "source": [
    "model_toxic_json = model.to_json()\n",
    "with open(\"model_toxic.json\", \"w\") as json_file:\n",
    "    json_file.write(model_toxic_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_toxic.h5\")\n",
    "print(\"Saved model_toxic to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 91s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#score on the test set! \n",
    "score = model.evaluate(test_data, y_test['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic label acc: 91.06%\n"
     ]
    }
   ],
   "source": [
    "print(\"toxic label %s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train models for all the other labels \n",
    "y_train\n",
    "labels = y_train.columns\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "====================================\n",
      "starting fit on toxic\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "starting fit on severe_toxic\n",
      "====================================\n",
      "====================================\n",
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/3\n",
      "111699/111699 [==============================] - 1719s 15ms/step - loss: 0.0356 - acc: 0.9901 - val_loss: 0.0246 - val_acc: 0.9904\n",
      "Epoch 2/3\n",
      "111699/111699 [==============================] - 1850s 17ms/step - loss: 0.0205 - acc: 0.9915 - val_loss: 0.0255 - val_acc: 0.9908\n",
      "Epoch 3/3\n",
      "111699/111699 [==============================] - 2209s 20ms/step - loss: 0.0147 - acc: 0.9936 - val_loss: 0.0293 - val_acc: 0.9900\n",
      "Saved severe_toxic to disk\n",
      "63978/63978 [==============================] - 81s 1ms/step\n",
      "severe_toxic acc: 98.44%\n",
      "====================================\n",
      "====================================\n",
      "done with severe_toxic starting next fit\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "starting fit on obscene\n",
      "====================================\n",
      "====================================\n",
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/3\n",
      "111699/111699 [==============================] - 2180s 20ms/step - loss: 0.0821 - acc: 0.9750 - val_loss: 0.0540 - val_acc: 0.9806\n",
      "Epoch 2/3\n",
      "111699/111699 [==============================] - 1891s 17ms/step - loss: 0.0431 - acc: 0.9841 - val_loss: 0.0536 - val_acc: 0.9810\n",
      "Epoch 3/3\n",
      "111699/111699 [==============================] - 1759s 16ms/step - loss: 0.0299 - acc: 0.9887 - val_loss: 0.0591 - val_acc: 0.9797\n",
      "Saved obscene to disk\n",
      "63978/63978 [==============================] - 86s 1ms/step\n",
      "obscene acc: 94.77%\n",
      "====================================\n",
      "====================================\n",
      "done with obscene starting next fit\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "starting fit on threat\n",
      "====================================\n",
      "====================================\n",
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/3\n",
      "111699/111699 [==============================] - 1750s 16ms/step - loss: 0.0201 - acc: 0.9967 - val_loss: 0.0120 - val_acc: 0.9973\n",
      "Epoch 2/3\n",
      "111699/111699 [==============================] - 1751s 16ms/step - loss: 0.0094 - acc: 0.9974 - val_loss: 0.0098 - val_acc: 0.9972\n",
      "Epoch 3/3\n",
      "111699/111699 [==============================] - 1830s 16ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0109 - val_acc: 0.9970\n",
      "Saved threat to disk\n",
      "63978/63978 [==============================] - 95s 1ms/step\n",
      "threat acc: 99.58%\n",
      "====================================\n",
      "====================================\n",
      "done with threat starting next fit\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "starting fit on insult\n",
      "====================================\n",
      "====================================\n",
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/3\n",
      "111699/111699 [==============================] - 1819s 16ms/step - loss: 0.0921 - acc: 0.9693 - val_loss: 0.0713 - val_acc: 0.9719\n",
      "Epoch 2/3\n",
      "111699/111699 [==============================] - 1828s 16ms/step - loss: 0.0556 - acc: 0.9782 - val_loss: 0.0728 - val_acc: 0.9735\n",
      "Epoch 3/3\n",
      "111699/111699 [==============================] - 1850s 17ms/step - loss: 0.0402 - acc: 0.9839 - val_loss: 0.0828 - val_acc: 0.9707\n",
      "Saved insult to disk\n",
      "63978/63978 [==============================] - 91s 1ms/step\n",
      "insult acc: 94.29%\n",
      "====================================\n",
      "====================================\n",
      "done with insult starting next fit\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "starting fit on identity_hate\n",
      "====================================\n",
      "====================================\n",
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/3\n",
      "111699/111699 [==============================] - 1735s 16ms/step - loss: 0.0396 - acc: 0.9914 - val_loss: 0.0303 - val_acc: 0.9913\n",
      "Epoch 2/3\n",
      "111699/111699 [==============================] - 1687s 15ms/step - loss: 0.0207 - acc: 0.9932 - val_loss: 0.0285 - val_acc: 0.9916\n",
      "Epoch 3/3\n",
      "111699/111699 [==============================] - 9192s 82ms/step - loss: 0.0119 - acc: 0.9956 - val_loss: 0.0356 - val_acc: 0.9904\n",
      "Saved identity_hate to disk\n",
      "63978/63978 [==============================] - 125s 2ms/step\n",
      "identity_hate acc: 97.70%\n",
      "====================================\n",
      "====================================\n",
      "done with identity_hate starting next fit\n",
      "====================================\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(\"====================================\")\n",
    "    print(\"====================================\")\n",
    "    print(\"starting fit on\", label)\n",
    "    print(\"====================================\")\n",
    "    print(\"====================================\")\n",
    "    if label == 'toxic':\n",
    "        continue #already trained toxic\n",
    "    \n",
    "    #just reset the model, bc idk how to make sure it deletes old fit\n",
    "    model = Sequential()\n",
    "    #first layer is embedding, takes in size of vocab, 100 dim embedding, and 150 which is length of the comment \n",
    "    model.add(Embedding(num_words, 100, input_length=150)) \n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    #fit the data to the next label \n",
    "    model.fit(data, np.array(y_train[label]), validation_split=.3, epochs=3)\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    name_json = label + '.json'\n",
    "    name_h5 = label + '.h5'\n",
    "    \n",
    "    #save into json file\n",
    "    with open(name_json, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(name_h5)\n",
    "    print(\"Saved\", label,\"to disk\")\n",
    "    \n",
    "    #lastly, evaluate on test \n",
    "    score = model.evaluate(test_data, y_test[label])\n",
    "    print(label,\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "    print(\"====================================\")\n",
    "    print(\"====================================\")\n",
    "    print(\"done with\", label,\"starting next fit\")\n",
    "    print(\"====================================\")\n",
    "    print(\"====================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#try # 2 dont use \n",
    "X_train['seq'] = tokenizer.texts_to_sequences(X_train['comment_text'])\n",
    "X_test['seq'] = tokenizer.texts_to_sequences(X_test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min length before: 1\n"
     ]
    }
   ],
   "source": [
    "def pad_shorter(arr, max_len):\n",
    "    for n in range(len(arr), max_len):\n",
    "        arr.append(None) \n",
    "    return arr\n",
    "\n",
    "max_len = 100\n",
    "print(\"min length before:\", X_train.seq.map(lambda x: len(x)).min())\n",
    "X_train['seq'] = X_train['seq'].apply(lambda x: pad_shorter(x, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min length after: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[689, 76, 2, 127, 131, 178, 30, 673, 4512, 120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[96146, 53, 2636, 14, 556, 3810, 74, 4557, 270...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>[413, 438, 74, 135, 15, 250, 3, 72, 315, 79, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>[58, 8, 229, 98, 55, 329, 1437, 16, 2134, 8, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>[7, 1678, 20, 30, 3517, 55, 1070, 7, 580, 40, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>[5, 13, 2, 428, 85, 4, 903, 83, 21, 314, 563, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>[7, 57, 17, 4653, 4, 207, 10, 9, 6, 3328, 232,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>[34279, 7331, 5209, 47, 738, 24, 13, 8168, 351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>[5, 12, 575, 50, 12, 25, 211, 7, 63, 202, 16, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>[5, 8, 135, 60, 68, 7, 253, 8, 588, 64, 5, 30,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...   \n",
       "1       D'aww! He matches this background colour I'm s...   \n",
       "2       Hey man, I'm really not trying to edit war. It...   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       You, sir, are my hero. Any chance you remember...   \n",
       "...                                                   ...   \n",
       "159566  \":::::And for the second time of asking, when ...   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  And it looks like it was actually you who put ...   \n",
       "159570  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "                                                      seq  \n",
       "0       [689, 76, 2, 127, 131, 178, 30, 673, 4512, 120...  \n",
       "1       [96146, 53, 2636, 14, 556, 3810, 74, 4557, 270...  \n",
       "2       [413, 438, 74, 135, 15, 250, 3, 72, 315, 79, 5...  \n",
       "3       [58, 8, 229, 98, 55, 329, 1437, 16, 2134, 8, 6...  \n",
       "4       [7, 1678, 20, 30, 3517, 55, 1070, 7, 580, 40, ...  \n",
       "...                                                   ...  \n",
       "159566  [5, 13, 2, 428, 85, 4, 903, 83, 21, 314, 563, ...  \n",
       "159567  [7, 57, 17, 4653, 4, 207, 10, 9, 6, 3328, 232,...  \n",
       "159568  [34279, 7331, 5209, 47, 738, 24, 13, 8168, 351...  \n",
       "159569  [5, 12, 575, 50, 12, 25, 211, 7, 63, 202, 16, ...  \n",
       "159570  [5, 8, 135, 60, 68, 7, 253, 8, 588, 64, 5, 30,...  \n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"min length after:\", X_train.seq.map(lambda x: len(x)).min())\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length before: 1403\n",
      "max length after: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[689, 76, 2, 127, 131, 178, 30, 673, 4512, 120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[96146, 53, 2636, 14, 556, 3810, 74, 4557, 270...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>[413, 438, 74, 135, 15, 250, 3, 72, 315, 79, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>[58, 8, 229, 98, 55, 329, 1437, 16, 2134, 8, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>[7, 1678, 20, 30, 3517, 55, 1070, 7, 580, 40, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "                                                 seq  \n",
       "0  [689, 76, 2, 127, 131, 178, 30, 673, 4512, 120...  \n",
       "1  [96146, 53, 2636, 14, 556, 3810, 74, 4557, 270...  \n",
       "2  [413, 438, 74, 135, 15, 250, 3, 72, 315, 79, 5...  \n",
       "3  [58, 8, 229, 98, 55, 329, 1437, 16, 2134, 8, 6...  \n",
       "4  [7, 1678, 20, 30, 3517, 55, 1070, 7, 580, 40, ...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trunc_longer(arr, max_len):\n",
    "    if len(arr) > max_len:\n",
    "        arr = arr[:max_len]\n",
    "    return arr\n",
    "\n",
    "max_len = 100\n",
    "print(\"max length before:\", X_train.seq.map(lambda x: len(x)).max())\n",
    "X_train['seq'] = X_train['seq'].apply(lambda x: trunc_longer(x, max_len))\n",
    "print(\"max length after:\", X_train.seq.map(lambda x: len(x)).max())\n",
    "X_train.head()\n",
    "\n",
    "#here all the comments should be the same size. go back and check what the mean/median \n",
    "#length of comment (by word) is, 100 might be a hyperparameter we should tune. \n",
    "#could also be checkign what the lengths are of toxic comments vs. normal comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, start the RNN model somehow \n",
    "sequences = tokenizer.texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-659ad5532fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sequences' is not defined"
     ]
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pandas datasets into a tensor\n",
    "# train = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "# test = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general notes:\n",
    "\n",
    "i feel like we should train the length of comment as a hyperparameter. need to figure out how long most comments are and set to that, not arbitrarily 150. find most common length, medial length, etc. worried that the current model is cutting off any hashtags at the end, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
