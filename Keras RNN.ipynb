{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import string\n",
    "import datetime\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#modeling imports\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#tf imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score\n",
    "\n",
    "\n",
    "#multinomial nb\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from scipy.sparse.linalg import svds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "subm = pd.read_csv('./data/sample_submission.csv')\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['none'] = 1-train[labels].max(axis=1) #make an indicator for when there is no \n",
    "                                            #value for any of the labels \n",
    "#cant have any of the unknown values \n",
    "train['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "test['comment_text'].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    data = os.path.join(\"data\", \"train.csv\")\n",
    "\n",
    "    df = pd.read_csv(data)\n",
    "    X_train = df[['comment_text']]\n",
    "    y_train = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "    X_test = pd.read_csv(os.path.join(\"data\", \"test.csv\"))\n",
    "    y_test = pd.read_csv(os.path.join(\"data\", \"test_labels.csv\"))\n",
    "    test = X_test.merge(y_test, on='id')\n",
    "    test = test[ (test['toxic']!=-1) | (test['severe_toxic']!=-1) | \n",
    "                (test['obscene']!=-1) | (test['threat']!=-1) | (test['insult']!=-1) \n",
    "                | (test['identity_hate']!=-1) ]\n",
    "    test = test.reset_index(drop=True) \n",
    "    \n",
    "    X_test = test[['comment_text']]\n",
    "    y_test = test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "import keras\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: should eventually take out the stop words here\n",
    "#train tokenizer, then encode documents (comment_text)\n",
    "tokenizer = Tokenizer(oov_token=True)\n",
    "tokenizer.fit_on_texts(X_train['comment_text']) #fit the tokenizer to training set, make the test unknown words be an UNK value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210339"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying to find the length of the vocabulary, might shrink\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "num_words\n",
    "#looks like there is about 210K words... maybe we should pass in a maxwords \n",
    "#argument to the ttokenizer, but my guess is words that are really making \n",
    "#toxic comments are less frequent, so don't necessarily want to do that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_train['comment_text'])\n",
    "data = pad_sequences(sequences, maxlen=150)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test['comment_text'])\n",
    "test_data = pad_sequences(test_sequences, maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network architecture\n",
    "# inspired at https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e and https://medium.com/@sabber/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b\n",
    "model = Sequential()\n",
    "#first layer is embedding, takes in size of vocab, 100 dim embedding, and 150 which is length of the comment \n",
    "model.add(Embedding(num_words, 100, input_length=150)) \n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0           0             0        0       0       0              0\n",
       "1           0             0        0       0       0              0\n",
       "2           0             0        0       0       0              0\n",
       "3           0             0        0       0       0              0\n",
       "4           0             0        0       0       0              0\n",
       "...       ...           ...      ...     ...     ...            ...\n",
       "159566      0             0        0       0       0              0\n",
       "159567      0             0        0       0       0              0\n",
       "159568      0             0        0       0       0              0\n",
       "159569      0             0        0       0       0              0\n",
       "159570      0             0        0       0       0              0\n",
       "\n",
       "[159571 rows x 6 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Isabella_GC/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/3\n",
      "111699/111699 [==============================] - 1872s 17ms/step - loss: 0.1388 - acc: 0.9529 - val_loss: 0.1044 - val_acc: 0.9621\n",
      "Epoch 2/3\n",
      "111699/111699 [==============================] - 2025s 18ms/step - loss: 0.0792 - acc: 0.9698 - val_loss: 0.1018 - val_acc: 0.9638\n",
      "Epoch 3/3\n",
      "111699/111699 [==============================] - 1793s 16ms/step - loss: 0.0533 - acc: 0.9795 - val_loss: 0.1214 - val_acc: 0.9602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2f46a400>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first run through didn't specify a batch size, probably do that\n",
    "#on the next try. \n",
    "model.fit(data, np.array(y_train['toxic']), validation_split=.3, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model_toxic to disk\n"
     ]
    }
   ],
   "source": [
    "model_toxic_json = model.to_json()\n",
    "with open(\"model_toxic.json\", \"w\") as json_file:\n",
    "    json_file.write(model_toxic_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_toxic.h5\")\n",
    "print(\"Saved model_toxic to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 91s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#score on the test set! \n",
    "score = model.evaluate(test_data, y_test['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic label acc: 91.06%\n"
     ]
    }
   ],
   "source": [
    "print(\"toxic label %s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train models for all the other labels \n",
    "y_train\n",
    "labels = y_train.columns\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "====================================\n",
      "starting fit on toxic\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "starting fit on severe_toxic\n",
      "====================================\n",
      "====================================\n",
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/3\n",
      " 11904/111699 [==>...........................] - ETA: 27:02 - loss: 0.0697 - acc: 0.9892"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(\"====================================\")\n",
    "    print(\"====================================\")\n",
    "    print(\"starting fit on\", label)\n",
    "    print(\"====================================\")\n",
    "    print(\"====================================\")\n",
    "    if label == 'toxic':\n",
    "        continue #already trained toxic\n",
    "    \n",
    "    #just reset the model, bc idk how to make sure it deletes old fit\n",
    "    model = Sequential()\n",
    "    #first layer is embedding, takes in size of vocab, 100 dim embedding, and 150 which is length of the comment \n",
    "    model.add(Embedding(num_words, 100, input_length=150)) \n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    #fit the data to the next label \n",
    "    model.fit(data, np.array(y_train[label]), validation_split=.3, epochs=3)\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    name_json = label + '.json'\n",
    "    name_h5 = label + '.h5'\n",
    "    \n",
    "    #save into json file\n",
    "    with open(name_json, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(name_h5)\n",
    "    print(\"Saved\", label,\"to disk\")\n",
    "    \n",
    "    #lastly, evaluate on test \n",
    "    score = model.evaluate(test_data, y_test[label])\n",
    "    print(label,\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "    print(\"====================================\")\n",
    "    print(\"====================================\")\n",
    "    print(\"done with\", label,\"starting next fit\")\n",
    "    print(\"====================================\")\n",
    "    print(\"====================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#try # 2 dont use \n",
    "X_train['seq'] = tokenizer.texts_to_sequences(X_train['comment_text'])\n",
    "X_test['seq'] = tokenizer.texts_to_sequences(X_test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min length before: 1\n"
     ]
    }
   ],
   "source": [
    "def pad_shorter(arr, max_len):\n",
    "    for n in range(len(arr), max_len):\n",
    "        arr.append(None) \n",
    "    return arr\n",
    "\n",
    "max_len = 100\n",
    "print(\"min length before:\", X_train.seq.map(lambda x: len(x)).min())\n",
    "X_train['seq'] = X_train['seq'].apply(lambda x: pad_shorter(x, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min length after: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[689, 76, 2, 127, 131, 178, 30, 673, 4512, 120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[96146, 53, 2636, 14, 556, 3810, 74, 4557, 270...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>[413, 438, 74, 135, 15, 250, 3, 72, 315, 79, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>[58, 8, 229, 98, 55, 329, 1437, 16, 2134, 8, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>[7, 1678, 20, 30, 3517, 55, 1070, 7, 580, 40, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>[5, 13, 2, 428, 85, 4, 903, 83, 21, 314, 563, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>[7, 57, 17, 4653, 4, 207, 10, 9, 6, 3328, 232,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>[34279, 7331, 5209, 47, 738, 24, 13, 8168, 351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>[5, 12, 575, 50, 12, 25, 211, 7, 63, 202, 16, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>[5, 8, 135, 60, 68, 7, 253, 8, 588, 64, 5, 30,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...   \n",
       "1       D'aww! He matches this background colour I'm s...   \n",
       "2       Hey man, I'm really not trying to edit war. It...   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       You, sir, are my hero. Any chance you remember...   \n",
       "...                                                   ...   \n",
       "159566  \":::::And for the second time of asking, when ...   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  And it looks like it was actually you who put ...   \n",
       "159570  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "                                                      seq  \n",
       "0       [689, 76, 2, 127, 131, 178, 30, 673, 4512, 120...  \n",
       "1       [96146, 53, 2636, 14, 556, 3810, 74, 4557, 270...  \n",
       "2       [413, 438, 74, 135, 15, 250, 3, 72, 315, 79, 5...  \n",
       "3       [58, 8, 229, 98, 55, 329, 1437, 16, 2134, 8, 6...  \n",
       "4       [7, 1678, 20, 30, 3517, 55, 1070, 7, 580, 40, ...  \n",
       "...                                                   ...  \n",
       "159566  [5, 13, 2, 428, 85, 4, 903, 83, 21, 314, 563, ...  \n",
       "159567  [7, 57, 17, 4653, 4, 207, 10, 9, 6, 3328, 232,...  \n",
       "159568  [34279, 7331, 5209, 47, 738, 24, 13, 8168, 351...  \n",
       "159569  [5, 12, 575, 50, 12, 25, 211, 7, 63, 202, 16, ...  \n",
       "159570  [5, 8, 135, 60, 68, 7, 253, 8, 588, 64, 5, 30,...  \n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"min length after:\", X_train.seq.map(lambda x: len(x)).min())\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length before: 1403\n",
      "max length after: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[689, 76, 2, 127, 131, 178, 30, 673, 4512, 120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[96146, 53, 2636, 14, 556, 3810, 74, 4557, 270...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>[413, 438, 74, 135, 15, 250, 3, 72, 315, 79, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>[58, 8, 229, 98, 55, 329, 1437, 16, 2134, 8, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>[7, 1678, 20, 30, 3517, 55, 1070, 7, 580, 40, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "                                                 seq  \n",
       "0  [689, 76, 2, 127, 131, 178, 30, 673, 4512, 120...  \n",
       "1  [96146, 53, 2636, 14, 556, 3810, 74, 4557, 270...  \n",
       "2  [413, 438, 74, 135, 15, 250, 3, 72, 315, 79, 5...  \n",
       "3  [58, 8, 229, 98, 55, 329, 1437, 16, 2134, 8, 6...  \n",
       "4  [7, 1678, 20, 30, 3517, 55, 1070, 7, 580, 40, ...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trunc_longer(arr, max_len):\n",
    "    if len(arr) > max_len:\n",
    "        arr = arr[:max_len]\n",
    "    return arr\n",
    "\n",
    "max_len = 100\n",
    "print(\"max length before:\", X_train.seq.map(lambda x: len(x)).max())\n",
    "X_train['seq'] = X_train['seq'].apply(lambda x: trunc_longer(x, max_len))\n",
    "print(\"max length after:\", X_train.seq.map(lambda x: len(x)).max())\n",
    "X_train.head()\n",
    "\n",
    "#here all the comments should be the same size. go back and check what the mean/median \n",
    "#length of comment (by word) is, 100 might be a hyperparameter we should tune. \n",
    "#could also be checkign what the lengths are of toxic comments vs. normal comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, start the RNN model somehow \n",
    "sequences = tokenizer.texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pandas datasets into a tensor\n",
    "# train = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "# test = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general notes:\n",
    "\n",
    "i feel like we should train the length of comment as a hyperparameter. need to figure out how long most comments are and set to that, not arbitrarily 150. find most common length, medial length, etc. worried that the current model is cutting off any hashtags at the end, etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
