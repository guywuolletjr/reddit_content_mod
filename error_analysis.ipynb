{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "preprocessing...\n",
      "lowercase\n",
      "contractions\n",
      "numbers\n",
      "newlines\n",
      "stopwords\n",
      "lemmas\n",
      "tokenization...\n",
      "221131 first!!!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.models import model_from_json\n",
    "from preprocc import *\n",
    "import numpy \n",
    "import os \n",
    "\n",
    "print(\"loading data...\")\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "print(\"preprocessing...\")\n",
    "X_train, X_test = preprocess(X_train, X_test)\n",
    "print(\"tokenization...\")\n",
    "train_data, test_data = tokenize(X_train, X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded lstm model from disk\n",
      "Loaded double model from disk\n",
      "Loaded triple model from disk\n",
      "Loaded bi lstm model from disk\n",
      "Loaded gru model from disk\n",
      "Loaded tfidf nn model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/eight_way.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "eight_way = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "eight_way.load_weights(\"/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/eight_way.h5\")\n",
    "print(\"Loaded lstm model from disk\")\n",
    " \n",
    "\n",
    "#doublelstm \n",
    "json_file = open('/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/double_lstm.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "double_lstm = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "double_lstm.load_weights(\"/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/double_lstm.h5\")\n",
    "print(\"Loaded double model from disk\")\n",
    "\n",
    "#triplelstm \n",
    "json_file = open('/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/triple_lstm.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "triple_lstm = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "triple_lstm.load_weights(\"/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/triple_lstm.h5\")\n",
    "print(\"Loaded triple model from disk\")\n",
    " \n",
    "\n",
    "#bilstm\n",
    "json_file = open('/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/bi_lstm.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "bi_lstm = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "bi_lstm.load_weights(\"/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/bi_lstm.h5\")\n",
    "print(\"Loaded bi lstm model from disk\")\n",
    "\n",
    "#gru\n",
    "json_file = open('/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/gru.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "gru = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "gru.load_weights(\"/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/gru.h5\")\n",
    "print(\"Loaded gru model from disk\")\n",
    "\n",
    "#tfidf\n",
    "json_file = open('/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/tfidf_nn_architecture.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "tfidf_nn = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "tfidf_nn.load_weights(\"/Users/Isabella_GC/documents/senioryear/cs230/reddit_content_mod/models/tfidf_nn_weights.h5\")\n",
    "print(\"Loaded tfidf nn model from disk\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LSTM\n",
      "63978/63978 [==============================] - 85s 1ms/step\n",
      "loss: 9.90%\n",
      "acc: 95.93%\n",
      "Evaluating double LSTM\n",
      "63978/63978 [==============================] - 157s 2ms/step\n",
      "loss: 9.90%\n",
      "acc: 95.93%\n",
      "Evaluating triple LSTM\n",
      "63978/63978 [==============================] - 234s 4ms/step\n",
      "loss: 9.90%\n",
      "acc: 95.93%\n",
      "Evaluating bidirectional LSTM\n",
      "63978/63978 [==============================] - 146s 2ms/step\n",
      "loss: 9.90%\n",
      "acc: 95.93%\n",
      "Evaluating GRU\n",
      "63978/63978 [==============================] - 68s 1ms/step\n",
      "loss: 9.90%\n",
      "acc: 95.93%\n",
      "Evaluating TFIDF NN\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (10000,) but got array with shape (150,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1b5029201801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating TFIDF NN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtfidf_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtf_idf_nn_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtfidf_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (10000,) but got array with shape (150,)"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "print(\"Evaluating LSTM\")\n",
    "eight_way.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "eight_way_score = eight_way.evaluate(test_data, y_test, verbose=1)\n",
    "for i in range(len(eight_way.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (eight_way.metrics_names[i], score[i]*100))\n",
    "\n",
    "print(\"Evaluating double LSTM\")\n",
    "double_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "double_lstm_score = double_lstm.evaluate(test_data, y_test, verbose=1)\n",
    "for i in range(len(double_lstm.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (double_lstm.metrics_names[i], score[i]*100))\n",
    "\n",
    "    \n",
    "print(\"Evaluating triple LSTM\")\n",
    "triple_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "triple_lstm_score = triple_lstm.evaluate(test_data, y_test, verbose=1)\n",
    "for i in range(len(triple_lstm.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (triple_lstm.metrics_names[i], score[i]*100))\n",
    "    \n",
    "\n",
    "print(\"Evaluating bidirectional LSTM\")\n",
    "bi_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bi_lstm_score = bi_lstm.evaluate(test_data, y_test, verbose=1)\n",
    "for i in range(len(bi_lstm.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (bi_lstm.metrics_names[i], score[i]*100))\n",
    "\n",
    "    \n",
    "print(\"Evaluating GRU\")\n",
    "gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "gru_score = gru.evaluate(test_data, y_test, verbose=1)\n",
    "for i in range(len(gru.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (gru.metrics_names[i], score[i]*100))\n",
    "\n",
    "\n",
    "print(\"Evaluating TFIDF NN\")\n",
    "tfidf_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tf_idf_nn_score = tfidf_nn.evaluate(X_test, y_test, verbose=1)\n",
    "for i in range(len(tfidf_nn.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (tfidf_nn.metrics_names[i], score[i]*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
